# import the libraries
!pip install timm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statistics import mean
import math

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import cv2

import timm
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, SGD
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import StepLR

import albumentations as A
from albumentations.pytorch import ToTensorV2

import warnings
warnings.filterwarnings('ignore')
# requesting acess for the GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# stroing the data in form mof dictionary using pandas
train = pd.read_csv('../input/concetto22/train.csv')
test = pd.read_csv('../input/concetto22/test.csv')
# This function adds an extra column having the path of the images
def append_path(df):
    target_str = []
    for i in range(len(df)):
        target_str.append(str(df['id'][i]))
    for i in range(len(df)):
        target_str[i] = target_str[i].replace('.0', '.tif') 
        target_str[i] = '/kaggle/input/concetto22/concetto_CDT/concetto_CDT/'+target_str[i]
    df['path'] = target_str
    return df
# calling the above function
train = append_path(train)
train.head()
test = append_path(test)
test.head()
plt.imshow(cv2.imread(train['path'][1]))
train_data = pd.merge(x_train, y_train, right_index=True, left_index=True)
val_data = pd.merge(x_val, y_val, right_index=True, left_index=True)
#defining a configuration

class CFG:
    model_name = 'resnet101'
    target_size = 6
    size = 264
    batch_size = 12
    epochs = 10
    num_workers = 2
    lr = 1e-4
    weight_decay = 1e-5
    train = True
    target_col = 'tar'
    # transforming our training data into required desired form along with the validation data
class TrainDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.file_names = df['path'].values
        self.labels = df['tar'].values
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.file_names[idx]
        file_path = file_name
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        label = torch.tensor(self.labels[idx]).long()
        return image, label
    

class TestDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.file_names = df['path'].values
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.file_names[idx]
        file_path = file_name
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        return image
        # Transforms and used to do data augment augmentation
def get_transforms(*, data):
    
    if data == 'train':
        return A.Compose([
            A.Resize(CFG.size, CFG.size),
            A.RandomResizedCrop(CFG.size, CFG.size),
            A.HorizontalFlip(p=0.5),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])

    elif data == 'valid':
        return A.Compose([
            A.Resize(CFG.size, CFG.size),
            A.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])
        class CustomNet(nn.Module):
    def __init__(self, model_name=CFG.model_name, pretrained=False):
        super().__init__()
        self.model = timm.create_model(CFG.model_name, pretrained=pretrained)
        #print(self.model.default_cfg["classifier"])
        n_features = self.model.fc.in_features #either fc or classifier , check using above line
        self.model.classifier = nn.Linear(n_features, CFG.target_size)

    def forward(self, x):
        x = self.model(x)
        return x
        model = CustomNet(model_name=CFG.model_name, pretrained=True)
        # loss function --- we suing cross entropy loss function
def get_criterion():
    criterion = nn.NLLLoss()
    return criterion
    # code to get accuracy
def get_score(y_true, y_pred):
    return accuracy_score(y_true, y_pred)
    # using ADAM optimizer
def get_optimizer(model):
    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)
    return optimizer

# learning rate scheduler/
def get_scheduler(optimizer):
    scheduler = StepLR(optimizer, step_size=2, gamma=0.1, verbose=True)
    return scheduler
    class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
        
def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):
    model.train() # switch to training mode
    running_loss = 0
    count = 0
    for (images, labels) in tqdm(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        y_preds = model(images)
        
        loss = criterion(y_preds, labels)
        running_loss += loss.item()*labels.shape[0]
        count += 1
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
    return running_loss/count


def valid_fn(valid_loader, model, criterion, device):
    model.eval() # switch to evaluation mode
    preds = []
    running_loss = 0
    count = 0
    
    for (images, labels) in tqdm(valid_loader):
        images = images.to(device)
        labels = labels.to(device)
        # compute loss
        with torch.no_grad():
            y_preds = model(images)
        loss = criterion(y_preds, labels)
        running_loss += loss.item()*labels.shape[0]
        count += 1
        # record accuracy
        preds.append(y_preds.softmax(1).to('cpu').numpy())
    predictions = np.concatenate(preds)
    
    return (running_loss/count), predictions


def test_fun(test_loader, model, device):
    model.eval()
    preds = []
    test_df = pd.DataFrame()
    for step, (images) in enumerate(test_loader):
        images = images.to(device)
        with torch.no_grad():
            y_preds = model(images)
        preds.append(y_preds.softmax(1).to('cpu').numpy())
    predictions = np.concatenate(preds)
    pred = predictions.argmax(1)
    return pred
    # Train loop
def train_loop(train_data, valid_data):
    
    # create dataset
    train_dataset = TrainDataset(train_data, transform=get_transforms(data='train'))
    valid_dataset = TrainDataset(valid_data, transform=get_transforms(data='valid'))

    # create dataloader
    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, 
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)
    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, 
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)

    # create model and transfer to device
    model = CustomNet(CFG.model_name, pretrained=True)
    model.to(device)
    
    # select optimizer, scheduler and criterion
    optimizer = get_optimizer(model)
    scheduler = get_scheduler(optimizer)
    criterion = get_criterion()

    best_score = -1.0
    best_loss = np.inf
    
    # start training
    for epoch in range(CFG.epochs):
        # train
        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)
        # validation
        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)
        plt.scatter(epoch, avg_loss, c='r')
        plt.scatter(epoch, avg_val_loss, c='b')
        # Adding legend, which helps us recognize the curve according to it's color
#         plt.legend()
  
#         # To load the display window
#         plt.show()
        
#       valid_labels = valid_folds[CFG.target_col].values
        valid_labels = valid_data['tar']
        
        scheduler.step()

        # scoring
        score = get_score(valid_labels, preds.argmax(1))
        print("score: ", score)

        # code for saving the best model
        if score > best_score:
            print('Score Improved')
            best_score = score
            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f}')
            torch.save({'model': model.state_dict(), 
                        'preds': preds,
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict()},
                        './'+f'{CFG.model_name}_best.pth')
    
    check_point = torch.load('./'+f'{CFG.model_name}_best.pth')
    valid_data['preds'] = check_point['preds'].argmax(1)

    return valid_data
    # main
def main():
    def get_result(result_df):
        preds = result_df['preds'].values
        labels = result_df[CFG.target_col].values
        score = get_score(labels, preds)
    
    if CFG.train: 
        # train
        df = train_loop(train_data, val_data)
        get_result(df)
        if __name__ == '__main__':
    main()
    test_dataset = TestDataset(test, transform=get_transforms(data='valid'))
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, 
                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)
check_point = torch.load('./'+f'{CFG.model_name}_best.pth')
model = CustomNet(CFG.model_name, pretrained=True)
model.to(device)
model.load_state_dict(check_point['model'])
pred = test_fun(test_loader, model, device)
test['tar'] = pred
submission_df = test.drop(['path'], axis=1)
submission_df.to_csv("solution3.csv", index=False)
submission_df
train1=pd.read_csv('../input/ensembles/ensemble1.csv')
train2=pd.read_csv('../input/ensembles/ensemble2.csv')
train3=pd.read_csv('../input/ensembles/ensemble3.csv')
train4=pd.read_csv('../input/ensembless/ensemble4.csv')
train5=pd.read_csv('../input/ensembles/ensemble1.csv')
train2.drop('id', inplace=True, axis=1)
train3.drop('id', inplace=True, axis=1)
train4.drop('id', inplace=True, axis=1)
train5.drop('id', inplace=True, axis=1)
train1['tar2'] = train2['tar']

train1['tar3'] = train3['tar']

train1['tar4'] = train4['tar']
train1['tar5'] = train5['tar']
tt=train1
tt.drop('id', inplace=True, axis=1)
tt=np.array(tt)
tt=tt.T
tt.shape
from scipy import stats as st
tt
A,B=st.mode(tt)
A=A.T

# ans=[]
# for i in range (0,l):
#     count =np.zeros((6,1))
#     for j in range (0,4):
#         count(tt[i][j])++
    
#     max=np.argmax(count)
#     ans.append(max)
np.zeros((6,1))
predd=pd.read_csv('../input/ensembles/ensemble1.csv')
predd
b=pd.DataFrame(A,columns=['tar'])
b
predd.drop('tar', inplace=True, axis=1)
predd['tar']=b
predd.shape
predd.to_csv("solution6.csv", index=False)
predd
train1=pd.read_csv('../input/ensembles/ensemble1.csv')
train2=pd.read_csv('../input/ensembles/ensemble2.csv')
train3=pd.read_csv('../input/ensembless/ensemble4.csv')
train2.drop('id', inplace=True, axis=1)
train3.drop('id', inplace=True, axis=1)
# train2.drop('id', inplace=True, axis=1)
train1['tar2']=train2['tar']
train1['tar3']=train3['tar']
tt=train1
tt.drop('id', inplace=True, axis=1)
tt=np.array(tt)
tt=tt.T
from scipy import stats as st
A,b=st.mode(tt)
A=A.T
A=pd.DataFrame(A,columns=['tar'])
train5=pd.read_csv('../input/ensembles/ensemble1.csv')
train5.drop('tar', inplace=True, axis=1)
train5['tar']=A
train5.to_csv("aaru.csv", index=False)
align
